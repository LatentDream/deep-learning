{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# IFT6135-A2022\n",
        "# Assignment 3: GAN Practical\n",
        "\n",
        "You must fill in your answers to various questions in this notebook, following which you must export this notebook to a Python file named `gan.py` and submit it on Gradescope.\n",
        "\n",
        "Only edit the functions specified in the PDF (and wherever marked â€“ `# WRITE CODE HERE`). Do not change definitions or edit the rest of the template, else the autograder will not work.\n",
        "\n",
        "**Make sure you request a GPU runtime!**"
      ],
      "metadata": {
        "id": "8y_h9goW_--j"
      },
      "id": "8y_h9goW_--j"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GAN Basics\n",
        "\n",
        "Generative Adversarial Autoencoders are generative models that are popularly used for unsupervised learning and are aimed at solving a two-player zero-sum game where a generator model is used to produce realistic samples which can fool a discriminator, which is tasked with distinguishing between real and fake samples. \n",
        "\n",
        "The formal objective for GANs can be seen as\n",
        "\n",
        "\\begin{align*}\n",
        "\\min_G \\max_D \\mathbb{E}_{x\\sim p_{data}} [\\log D(x)] + \\mathbb{E}_{z\\sim p_z} [\\log (1 - D(G(z))]\n",
        "\\end{align*}\n",
        "\n",
        "Where we can see that the job of the discriminator is to distinguish between data coming from the real distribution $x_{real} \\sim p_{data}$ and the data that is being generated by the generator $x_{fake} = G(z)$ where $z\\sim p_z$, where $p_z$ is just some prior distribution, often kept as $\\mathcal{N}(0, I)$. One can see the objective for $D(\\cdot)$ through the lens of classification, and in particular through the binary cross entropy loss.\n",
        "\n",
        "On the other hand, once the discriminator is trained to optimality, the job of the generator (the outer $\\min$) is to generate samples that fool the discriminator. In practice, instead of training the discriminator to optimality, we just alternate between training the discriminator for one step, and then the generator for one step, and we keep alternating. Further, the objective for the generator leads to poor gradient properties, and hence for training of the generator we aim to instead maximize\n",
        "\n",
        "\\begin{align*}\n",
        "\\mathbb{E}_{z\\sim p_z} [\\log D(G(z))]\n",
        "\\end{align*}\n",
        "\n",
        "Note that in both the equations, $G(z)$ is optimal only if it is able to fool the discriminator.\n",
        "\n",
        "For details about GANs, please refer to [Goodfellow's Paper](https://arxiv.org/abs/1406.2661)."
      ],
      "metadata": {
        "id": "AdbBo0_tQp8y"
      },
      "id": "AdbBo0_tQp8y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1f2d714"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U datasets matplotlib==3.1.1 tqdm\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from datasets import load_dataset\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "\n",
        "from torchvision.utils import make_grid, save_image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torchvision.transforms import Compose, ToTensor, Lambda, ToPILImage, CenterCrop, Resize\n",
        "from torchvision import transforms\n",
        "\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def fix_experiment_seed(seed=0):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "fix_experiment_seed()\n",
        "\n",
        "results_folder = Path(\"./results\")\n",
        "results_folder.mkdir(exist_ok = True)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "id": "a1f2d714"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up the hyperparameters\n",
        "- Batch Size\n",
        "- Latent Dimensionality\n",
        "- Learning Rate"
      ],
      "metadata": {
        "id": "bK_NDpHdO_fn"
      },
      "id": "bK_NDpHdO_fn"
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Hyperparameters\n",
        "batch_size = 64   # Batch Size\n",
        "z_dim = 32        # Latent Dimensionality\n",
        "gen_lr = 1e-4     # Learning Rate for the Generator\n",
        "disc_lr = 1e-4    # Learning Rate for the Discriminator"
      ],
      "metadata": {
        "id": "5m7U3O5XC2i2"
      },
      "id": "5m7U3O5XC2i2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Dataset Statistics\n",
        "image_size = 32\n",
        "input_channels = 3\n",
        "\n",
        "# Resize and Normalize the Data\n",
        "transform = Compose([\n",
        "            transforms.Resize((image_size, image_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Lambda(lambda t: (t * 2) - 1)\n",
        "])\n",
        "\n",
        "# Helper Functions\n",
        "def show_image(image, nrow=8):\n",
        "  # Input: image\n",
        "  # Displays the image using matplotlib\n",
        "  grid_img = make_grid(image.detach().cpu(), nrow=nrow, padding=0)\n",
        "  plt.imshow(grid_img.permute(1, 2, 0))\n",
        "  plt.axis('off')\n",
        "\n",
        "def transforms_examples(examples):\n",
        "  # Helper function to perform transformations on the input images\n",
        "  if \"image\" in examples:\n",
        "     examples[\"pixel_values\"] = [transform(image) for image in examples[\"image\"]]\n",
        "     del examples[\"image\"]\n",
        "  else:\n",
        "     examples[\"pixel_values\"] = [transform(image) for image in examples[\"img\"]]\n",
        "     del examples[\"img\"]\n",
        "\n",
        "  return examples"
      ],
      "metadata": {
        "id": "lcwXsoUDEPwe"
      },
      "id": "lcwXsoUDEPwe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6134d691"
      },
      "outputs": [],
      "source": [
        "# Load dataset from the hub, normalize it and create the dataloader\n",
        "def get_dataloaders():\n",
        "  dataset = load_dataset(\"svhn\", 'cropped_digits', cache_dir='./data')\n",
        "  transformed_dataset = dataset.with_transform(transforms_examples)\n",
        "  train_dataloader = DataLoader(transformed_dataset[\"train\"], batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "  test_dataloader = DataLoader(transformed_dataset[\"test\"], batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "  return train_dataloader, test_dataloader"
      ],
      "id": "6134d691"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize the Data\n",
        "\n",
        "Lets visualize what our data actually looks like! We are using the [SVHN dataset](http://ufldl.stanford.edu/housenumbers/) which comprises of images of house numbers seen from the streets."
      ],
      "metadata": {
        "id": "HfRQis_EP-yh"
      },
      "id": "HfRQis_EP-yh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52e8273b"
      },
      "outputs": [],
      "source": [
        "# Visualize the Dataset\n",
        "def visualize():\n",
        "  train_dataloader, _ = get_dataloaders()\n",
        "  batch = next(iter(train_dataloader))\n",
        "  print(batch['pixel_values'].shape)\n",
        "\n",
        "  save_image((batch['pixel_values'] + 1.) * 0.5, './results/orig.png')\n",
        "  show_image((batch['pixel_values'] + 1.) * 0.5)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  visualize()"
      ],
      "id": "52e8273b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the Model Architectures\n",
        "\n",
        "For a GAN model, we need two architectures, one for the Generator and the other for the discriminator. The generator a sample from $\\mathcal{N}(0, I)$ to the input space (which is the pixel space), while the discriminator maps a sample in the input space to a scalar, which determines (after a sigmoid) the probability of the input being from the real distribution.\n",
        "\n",
        "The architectures for both the generator and the discriminator networks have been provided to you for ease of experimentation. You are welcome to play with the architecture and change it for exploration purposes, but please stick to this architecture for the purpose of this homework."
      ],
      "metadata": {
        "id": "PSzDGCJvQPvm"
      },
      "id": "PSzDGCJvQPvm"
    },
    {
      "cell_type": "code",
      "source": [
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "        \n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_dim, channels, generator_features=32):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.ConvTranspose2d(z_dim, generator_features * 4, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(generator_features * 4),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(generator_features * 4, generator_features * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(generator_features * 2),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d( generator_features * 2, generator_features * 1, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(generator_features * 1),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d( generator_features * 1, channels, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.apply(weights_init)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.model(input)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, channels, discriminator_features=32):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(channels, discriminator_features, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(discriminator_features, discriminator_features * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(discriminator_features * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(discriminator_features * 2, discriminator_features * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(discriminator_features * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(discriminator_features * 4, 1, 4, 1, 0, bias=False),\n",
        "        )\n",
        "\n",
        "        self.apply(weights_init)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.model(input)"
      ],
      "metadata": {
        "id": "812V5rIcHaaf"
      },
      "id": "812V5rIcHaaf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we define both the generator and the discriminator networks."
      ],
      "metadata": {
        "id": "2TprgdjZvoKm"
      },
      "id": "2TprgdjZvoKm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5126e21"
      },
      "outputs": [],
      "source": [
        "generator = Generator(z_dim, input_channels).to(device)\n",
        "discriminator = Discriminator(input_channels).to(device)"
      ],
      "id": "a5126e21"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set the optimizers for each model\n",
        "\n",
        "Your task is to set each optimizer to be the Adam optimizer with the corresponding parameters, the learning rates kept as disc_lr and gen_lr respectively, and the betas hyperparameter set as (0.5, 0.999). You can use PyTorch's in-built Adam optimizer.\n"
      ],
      "metadata": {
        "id": "1Wmd1FIEFo9U"
      },
      "id": "1Wmd1FIEFo9U"
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator_optimizer = None    # WRITE CODE HERE\n",
        "generator_optimizer = None        # WRITE CODE HERE"
      ],
      "metadata": {
        "id": "6T6vghqe-MFW"
      },
      "id": "6T6vghqe-MFW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the criterion required for training.\n",
        "\n",
        "Your task is to identify the objective as a binary cross entropy objective and fill in the corresponding criterion. It should take as input un-normalized probabilities (without the sigmoid) and the true labels, and return the averaged loss. You can use one of PyTorch's in-built criterions."
      ],
      "metadata": {
        "id": "2El5NXqZFs3I"
      },
      "id": "2El5NXqZFs3I"
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = None    # WRITE CODE HERE"
      ],
      "metadata": {
        "id": "2eDTpjMm-JSj"
      },
      "id": "2eDTpjMm-JSj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the Training functions for the discriminator and the generator\n",
        "\n",
        "Now, we need training scripts to run one step of discriminator training and one step of generator training respectively. In particular, the task is to fill the following functions:\n",
        "\n",
        "- discriminator_train: The function takes as input a set of real samples and a set of fake samples, and is supposed to return the average loss for training of the discriminator. To recap, the objective for the discriminator is $\\max_D \\sum\\limits_{i=1}^N \\log D(x_i) + \\sum\\limits_{i=1}^N \\log (1 - D(G(z_i))$, where minimization implies just taking negative of the objective. Here $x_i$ are the real samples while $z_i$ are noise samples used to generate fake samples.\n",
        "\n",
        "- generator_train: The function takes as input a set of fake samples and is supposed to return the average loss for training of the generator. To recap, the objective for the generator is $\\max_G \\sum\\limits_{i=1}^N \\log D(G(z_i))$, where minimization implies just taking negative of the objective. Here $z_i$ are noise samples used to generate fake samples."
      ],
      "metadata": {
        "id": "XXqJc4_XGquo"
      },
      "id": "XXqJc4_XGquo"
    },
    {
      "cell_type": "code",
      "source": [
        "def discriminator_train(discriminator, generator, real_samples, fake_samples):\n",
        "  # Takes as input real and fake samples and returns the loss for the discriminator\n",
        "  # Inputs:\n",
        "  #   real_samples: Input images of size (batch_size, 3, 32, 32)\n",
        "  #   fake_samples: Input images of size (batch_size, 3, 32, 32)\n",
        "  # Returns:\n",
        "  #   loss: Discriminator loss\n",
        "\n",
        "  ones = None   # WRITE CODE HERE (targets for real data)\n",
        "  zeros = None  # WRITE CODE HERE (targets for fake data)\n",
        "\n",
        "  real_output = None    # WRITE CODE HERE (output of discriminator on real data)\n",
        "  fake_output = None    # WRITE CODE HERE (output of discriminator on fake data)\n",
        "\n",
        "  loss = None           # WRITE CODE HERE (define the loss based on criterion and above variables)\n",
        "\n",
        "  return loss\n",
        "\n",
        "def generator_train(discriminator, generator, fake_samples):\n",
        "  # Takes as input fake samples and returns the loss for the generator\n",
        "  # Inputs:\n",
        "  #   fake_samples: Input images of size (batch_size, 3, 32, 32)\n",
        "  # Returns:\n",
        "  #   loss: Generator loss\n",
        "\n",
        "  ones = None   # WRITE CODE HERE (targets for fake data but for generator loop)\n",
        "\n",
        "  output = None # WRITE CODE HERE (output of the discriminator on the fake data)\n",
        "\n",
        "  loss = None   # WRITE CODE HERE (loss for the generator based on criterion and above variables)\n",
        "\n",
        "  return loss\n",
        "\n",
        "def sample(generator, num_samples):\n",
        "  # Takes as input the number of samples and returns that many generated samples\n",
        "  # Inputs:\n",
        "  #   num_samples: Scalar denoting the number of samples\n",
        "  # Returns:\n",
        "  #   samples: Samples generated; tensor of shape (num_samples, 3, 32, 32)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    # WRITE CODE HERE (sample from p_z and then generate samples from it)\n",
        "    pass"
      ],
      "metadata": {
        "id": "fx3SbSTK-rPM"
      },
      "id": "fx3SbSTK-rPM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7444b0b"
      },
      "source": [
        "Finally, let's start training!\n",
        "Visualization of the samples generated, the original dataset and the reconstructions are saved locally in the notebook!"
      ],
      "id": "f7444b0b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92b12ed1"
      },
      "outputs": [],
      "source": [
        "epochs = 25\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  train_dataloder, _ = get_dataloaders()\n",
        "  for epoch in range(epochs):\n",
        "    with tqdm(train_dataloader, unit=\"batch\", leave=False) as tepoch:\n",
        "      for batch in tepoch:\n",
        "        tepoch.set_description(f\"Epoch: {epoch}\")\n",
        "\n",
        "        noise = torch.randn(batch_size, z_dim, 1, 1, device=device)\n",
        "        fake = generator(noise)\n",
        "        real = batch['pixel_values'].to(device)\n",
        "\n",
        "        discriminator.zero_grad()\n",
        "        disc_loss = discriminator_train(discriminator, generator, real, fake.detach())\n",
        "        disc_loss.backward()\n",
        "        discriminator_optimizer.step()\n",
        "\n",
        "        generator.zero_grad()\n",
        "        gen_loss = generator_train(discriminator, generator, fake)\n",
        "        gen_loss.backward()\n",
        "        generator_optimizer.step()\n",
        "\n",
        "        tepoch.set_postfix(disc_loss=disc_loss.item(), gen_loss=gen_loss.item())\n",
        "\n",
        "    samples = sample(generator, 64)\n",
        "    save_image((real + 1.) * 0.5, './results/orig.png')\n",
        "    save_image((samples + 1.) * 0.5, f'./results/samples_{epoch}.png')\n",
        "\n",
        "  show_image((samples + 1.) * 0.5)"
      ],
      "id": "92b12ed1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also visualize the interpolation between two points in the latent space: $z_1$ and $z_2$ by choosing points at equal intervals on the line from the two points."
      ],
      "metadata": {
        "id": "3PW1Xgz6-rbp"
      },
      "id": "3PW1Xgz6-rbp"
    },
    {
      "cell_type": "code",
      "source": [
        "def interpolate(generator, z_1, z_2, n_samples):\n",
        "  # Interpolate between z_1 and z_2 with n_samples number of points, with the first point being z_1 and last being z_2.\n",
        "  # Inputs:\n",
        "  #   z_1: The first point in the latent space\n",
        "  #   z_2: The second point in the latent space\n",
        "  #   n_samples: Number of points interpolated\n",
        "  # Returns:\n",
        "  #   sample: A sample from the generator obtained from each point in the latent space\n",
        "  #           Should be of size (n_samples, 3, 32, 32)\n",
        "\n",
        "  # WRITE CODE HERE (interpolate z_1 to z_2 with n_samples points and then)\n",
        "  # WRITE CODE HERE (    generate samples from the respective latents     )\n",
        "  \n",
        "  return None\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  z_1 = torch.randn(1, z_dim, 1 ,1).to(device)\n",
        "  z_2 = torch.randn(1, z_dim, 1, 1).to(device)\n",
        "\n",
        "  interp = interpolate(generator, z_1, z_2, 10)\n",
        "  show_image((interp + 1.) * 0.5, nrow=10)"
      ],
      "metadata": {
        "id": "-b47g2Bj7IF-"
      },
      "id": "-b47g2Bj7IF-",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}