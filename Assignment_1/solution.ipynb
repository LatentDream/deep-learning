{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guthi1/deep-learning/blob/main/Assignment_1/solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IFT6135-A2022\n",
        "# Assignment 1: Practical\n",
        "\n",
        "You must fill in your answers to various questions in this notebook, following which you must export this notebook to a Python file named `solution.py` and submit it on Gradescope.\n",
        "\n",
        "Only edit the functions specified in the PDF (and wherever marked â€“ `# WRITE CODE HERE`). Do not change definitions or edit the rest of the template, else the autograder will not work."
      ],
      "metadata": {
        "id": "nWJnNLufy2Yl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Make sure you request a GPU runtime (especially for Question 3)!**"
      ],
      "metadata": {
        "id": "SYa2b_UaRdZL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Chq23hzpyqGB"
      },
      "outputs": [],
      "source": [
        "# DO NOT MODIFY!\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Fix random seed\n",
        "random.seed(0)\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1: Implementing MLPs with NumPy (30 points)"
      ],
      "metadata": {
        "id": "_vP6_RXx02jN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NN(object):\n",
        "  \"\"\"\n",
        "    Implements an MLP.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               dims=(784, 128, 64, 10), # h_0, h_1, h_2, h_3\n",
        "               activation=\"relu\",       # Activation function\n",
        "               epsilon=1e-6,            # Correction factor\n",
        "               lr=0.01,                 # Learning rate\n",
        "               seed=0                   # Random seed\n",
        "              ):\n",
        "    \"\"\"\n",
        "      Constructor of the NN class.\n",
        "\n",
        "      dims: list or tuple or np.array, default (784, 128, 64, 10)\n",
        "        Values of h_0 (no. of features), h_1 (hidden dim. 1), h_2 (hidden dim. 2), h_3 (no. of output classes).\n",
        "      activation: string, default \"relu\"\n",
        "        Activation function to use.\n",
        "      epsilon: float or double, default 1e-6\n",
        "        Correction factor to clip probabilities.\n",
        "      lr: float or double, default 0.01\n",
        "        Learning rate for weight updates.\n",
        "      seed: int, default 0\n",
        "        Random seed.\n",
        "    \"\"\"\n",
        "    super(NN, self).__init__()\n",
        "\n",
        "    self.dims = dims\n",
        "    self.n_hidden = len(dims) - 2\n",
        "    self.activation_str = activation\n",
        "    self.epsilon = epsilon\n",
        "    self.lr = lr\n",
        "    self.seed = seed\n",
        "    self.debug = False\n",
        "\n",
        "  def initialize_weights(self):\n",
        "    \"\"\"\n",
        "      Results: Initializes the weights of the MLP from uniform(-1/sqrt(h_0), 1/sqrt(h_0)) and the biases to zeros.\n",
        "    \"\"\"\n",
        "    if self.seed is not None:\n",
        "      np.random.seed(self.seed)\n",
        "\n",
        "    self.weights = {}\n",
        "    # self.weights is a dictionary with keys W1, b1, W2, b2, ..., Wm, Bm where m - 1 is the number of hidden layers\n",
        "    # The keys W1, W2, ..., Wm correspond to weights while b1, b2, ..., bm correspond to biases\n",
        "    for layer_n in range(1, self.n_hidden + 2):\n",
        "      \"\"\" h_0 = nb. feature input data \"\"\"\n",
        "      h_0 = self.dims[0]\n",
        "\n",
        "      \"\"\" Number of weight of a layer \"\"\"\n",
        "      n_weights = self.dims[layer_n-1] * (self.dims[layer_n])\n",
        "\n",
        "      \"\"\" Init the weight \"\"\"\n",
        "      weights = np.random.uniform(-1/np.sqrt(h_0), 1/np.sqrt(h_0), size=n_weights)\n",
        "\n",
        "      \"\"\" Reshape in the good size\n",
        "      \" | W_00  W_01 .. |\n",
        "      \" | W_10  W_11 .. |\n",
        "      \" |  ..    ..  .. | \n",
        "      \"\"\"\n",
        "      weights = np.reshape(weights, (self.dims[layer_n-1], self.dims[layer_n]))\n",
        "\n",
        "      \"\"\" Weights : matrix of shape(n_neurone{layer_i}, n_neuro{layer_i-1}) \"\"\"\n",
        "      self.weights[f\"W{layer_n}\"] = weights\n",
        "      self.weights[f\"b{layer_n}\"] = np.zeros((1, self.dims[layer_n]))\n",
        "\n",
        "  def relu(self, x, grad=False):\n",
        "    \"\"\"\n",
        "      x: np.array\n",
        "        Inputs to calculate ReLU(x) for. x may contain a batch of inputs!\n",
        "      grad: bool, default False\n",
        "        If True, return the gradient of the activation with respect to the inputs to the function.\n",
        "\n",
        "      Outputs: Implements the ReLU activation function or its gradient.\n",
        "    \"\"\"\n",
        "    if grad:\n",
        "      x[x <= 0] = 0\n",
        "      x[x > 0] = 1\n",
        "      return x\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "  def sigmoid(self, x, grad=False):\n",
        "    \"\"\"\n",
        "      x: np.array\n",
        "        Inputs to calculate sigmoid(x) for. x may contain a batch of inputs!\n",
        "      grad: bool, default False\n",
        "        If True, return the gradient of the activation with respect to the inputs to the function.\n",
        "\n",
        "      Outputs: Implements the Sigmoid activation function or its gradient.\n",
        "    \"\"\"\n",
        "    def sigmoid(x):\n",
        "        return 1/(1+np.exp(-x))\n",
        "\n",
        "    if grad:\n",
        "      return sigmoid(x)*(1-sigmoid(x))\n",
        "    return sigmoid(x)\n",
        "\n",
        "  def tanh(self, x, grad=False):\n",
        "    \"\"\"\n",
        "      x: np.array\n",
        "        Inputs to calculate tanh(x) for. x may contain a batch of inputs!\n",
        "      grad: bool, default False\n",
        "        If True, return the gradient of the activation with respect to the inputs to the function.\n",
        "\n",
        "      Outputs: Implements the tanh activation function or its gradient.\n",
        "    \"\"\"\n",
        "    def tanh(x):\n",
        "        return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
        "\n",
        "    if grad:\n",
        "      return 1-(np.power(tanh(x), 2))\n",
        "    return tanh(x)\n",
        "\n",
        "  def activation(self, x, grad=False):\n",
        "    \"\"\"\n",
        "      x: np.array\n",
        "        Inputs to calculate activation(x) for. x may contain a batch of inputs!\n",
        "      grad: bool, default False\n",
        "        If True, return the gradient of the activation with respect to the inputs to the function.\n",
        "\n",
        "      Outputs: Returns the value of the activation or the gradient.\n",
        "    \"\"\"\n",
        "    if self.activation_str == \"relu\":\n",
        "      return self.relu(x, grad=grad) \n",
        "    elif self.activation_str == \"sigmoid\":\n",
        "      return self.sigmoid(x, grad=grad)\n",
        "    elif self.activation_str == \"tanh\":\n",
        "      return self.tanh(x, grad=grad)\n",
        "    else:\n",
        "      raise Exception(\"Invalid activation\")\n",
        "    return 0\n",
        "\n",
        "  def softmax(self, x):\n",
        "    \"\"\"\n",
        "      x: np.array\n",
        "        Inputs to calculate softmax over. x may contain a batch of inputs!\n",
        "\n",
        "      Outputs: Implements the softmax function, returns the array containing softmax(x).\n",
        "    \"\"\"\n",
        "    axis = 1\n",
        "    max = np.max(x, axis=axis)\n",
        "    exp_x = np.exp(x - np.expand_dims(max, axis))\n",
        "    sum = np.sum(exp_x, axis=axis)\n",
        "    return exp_x / np.expand_dims(sum, axis)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "      x: np.array\n",
        "        Inputs to the MLP. Note that x may contain multiple input examples, not just one example.\n",
        "\n",
        "      Outputs: Implements the forward pass, returns cache as described below.\n",
        "    \"\"\"\n",
        "    cache = {\"Z0\": x}\n",
        "    # cache is a dictionary with keys Z0, A1, Z1, ..., Am, Zm where m - 1 is the number of hidden layers\n",
        "    # Z0 just contains the inputs x to the network\n",
        "    # Ai corresponds to the preactivation at layer i, Zi corresponds to the activation at layer i\n",
        "    if self.debug:print(\"\\nFoward ...\")\n",
        "    z = x\n",
        "\n",
        "    for layer_n in range(1, self.n_hidden + 2):\n",
        "\n",
        "        # Get the weiths\n",
        "        weights = self.weights[f\"W{layer_n}\"]\n",
        "        bias = self.weights[f\"b{layer_n}\"]\n",
        "\n",
        "        # Calcul       \n",
        "        if self.debug:print(f\"layer_{layer_n}\")\n",
        "        if self.debug:print(f\"    z_{z.shape} dot w_{weights.shape} + b_{bias.shape}\")\n",
        "        # Do calcul\n",
        "        a = z.dot(weights) + bias\n",
        "        \n",
        "        if layer_n == self.n_hidden+1:\n",
        "            z = self.softmax(a)\n",
        "        else:\n",
        "            z = self.activation(a)\n",
        "        if self.debug:print(f\"    = z_{z.shape}\")\n",
        "    \n",
        "        # Save value\n",
        "        cache[f\"A{layer_n}\"] = a\n",
        "        cache[f\"Z{layer_n}\"] = z\n",
        "    \n",
        "    return cache\n",
        "\n",
        "  def loss(self, prediction, labels):\n",
        "    \"\"\"\n",
        "      prediction: np.array\n",
        "        Predicted probabilities for each class for inputs. May contain multiple examples (a batch)!\n",
        "      labels: np.array\n",
        "        True labels corresponding to the inputs (assume they are one-hot encoded). May contain multiple examples (a batch)!\n",
        "\n",
        "      Outputs: Returns the crossentropy loss (take the mean over number of inputs).\n",
        "    \"\"\"\n",
        "    prediction[np.where(prediction < self.epsilon)] = self.epsilon\n",
        "    prediction[np.where(prediction > 1 - self.epsilon)] = 1 - self.epsilon\n",
        "    return (- np.sum(labels * np.log(prediction))) / len(prediction)\n",
        "\n",
        "  def backward(self, cache, labels):\n",
        "    \"\"\"\n",
        "      cache: np.array\n",
        "        Results of the backward pass. This may be for multiple examples (a batch).\n",
        "      labels: np.array\n",
        "        True labels corresponding to the inputs in cache. May contain multiple examples (a batch)!\n",
        "\n",
        "      Outputs: Implements the backward pass, returns grads as described below.\n",
        "    \"\"\"\n",
        "    output = cache[f\"Z{self.n_hidden + 1}\"]\n",
        "    grads = {}\n",
        "\n",
        "    # grads is a dictionary with keys dAm, dWm, dbm, dZ(m-1), dA(m-1), ..., dW1, db1\n",
        "    # Remember to account for the number of input examples!\n",
        "\n",
        "    \"\"\"\n",
        "    We need to find: dC/dw et dC/db. For that, let introduce intermediate quantity\n",
        "    delta_l: [] vector of errors associated with neuron in layer l\n",
        "        -> delta_l = dC/ da\n",
        "        -> Come from activation(a_l + little_delta)\n",
        "        -> delta_l = dC/dz_l = activation'(a_l)\n",
        "    \"\"\"\n",
        "    if self.debug:print(\"\\nBackward ...\")\n",
        "    \n",
        "    # Last layer\n",
        "    \"\"\" delta_L = nabla Hadamard_product act_func_derivative(w_L * h_L-1 + b) \"\"\"\n",
        "    if self.debug:print(f\"last layer ... layer_{self.n_hidden + 1} ----------\")\n",
        "    if self.debug:print(f\"   -> delta\")\n",
        "    if self.debug:print(f\"      delta = pred_{output.shape} - labels_{labels.shape}\")\n",
        "    delta = (output - labels) # softmax cross_entropy backward\n",
        "    bias_grad = np.mean(delta, axis=0)\n",
        "    if self.debug:print(f\"      delta_{delta.shape}\")\n",
        "    # Save value\n",
        "    grads[f\"db{self.n_hidden + 1}\"] = bias_grad\n",
        "    grads[f\"dA{self.n_hidden + 1}\"] = delta\n",
        "\n",
        "    \"\"\" dC/dw_L = h_L-1 * delta_L \"\"\"\n",
        "    if self.debug:print(f\"   -> Update\")\n",
        "    z_in = cache[f\"Z{self.n_hidden}\"]\n",
        "    z_in = np.mean(z_in, axis=0)# TODO: FIX\n",
        "    if self.debug:print(f\"      partial_derivatire = z_in_{z_in.shape} * delta_{delta.shape}\")\n",
        "    partial_derivatire = np.multiply.outer(z_in, delta)\n",
        "    partial_derivatire = np.mean(partial_derivatire, axis=1) # TODO: FIX\n",
        "    if self.debug:print(f\"      partial_derivatire_{partial_derivatire.shape}\")\n",
        "    # Save value\n",
        "    grads[f\"dW{self.n_hidden + 1}\"] = partial_derivatire\n",
        "\n",
        "    delta = np.mean(delta, axis=0) # TODO: REMOVE\n",
        "\n",
        "\n",
        "    # From here, we need to apply the chain rule -  Loop backward\n",
        "    for layer_n in range(self.n_hidden, 0, -1):\n",
        "        if self.debug:print(f\"Looping backward ... layer_{layer_n} ----------\")\n",
        "        \"\"\" delta_l = ((w_l+1).T * delta_l+1) element_wise_mul act_func_derivative(w_l * z_l-1 + b) \"\"\"\n",
        "        if self.debug:print(f\"   -> delta\")\n",
        "        a = cache[f\"A{layer_n}\"]\n",
        "        activation_back = self.activation(a, grad=True)\n",
        "        activation_back = np.mean(activation_back, axis=0) # TODO: REMOVE\n",
        "        # Find error (delta) for the current layer\n",
        "        weights_next = self.weights[f\"W{layer_n+1}\"]\n",
        "        if self.debug:print(f\"      delta_pre_act = weight_{weights_next.T.shape} * delta_{delta.shape}\")\n",
        "        delta_pre_act = np.matmul(weights_next, delta)\n",
        "        grads[f\"dA{layer_n}\"] = delta_pre_act\n",
        "        # error * activation_back\n",
        "        if self.debug:print(f\"      delta_ = delta_pre_act_{delta_pre_act.shape} * activation_back_{activation_back.shape}\")\n",
        "        delta = delta_pre_act * activation_back\n",
        "        if self.debug:print(f\"      delta_{delta.shape}\")\n",
        "\n",
        "        grads[f\"db{layer_n}\"] = delta\n",
        "\n",
        "        \"\"\" dC/dw_l = z_l-1 * delta_l \"\"\"\n",
        "        if self.debug:print(f\"   -> Update\")\n",
        "        z_in = cache[f\"Z{layer_n-1}\"]\n",
        "        z_in = np.mean(z_in, axis=0)\n",
        "        if self.debug:print(f\"      partial_derivatire = z_in_{z_in.shape} * delta_{delta.shape}\")\n",
        "        partial_derivatire = np.multiply.outer(z_in, delta)\n",
        "        if self.debug:print(f\"      partial_derivatire_{partial_derivatire.shape}\")\n",
        "\n",
        "        grads[f\"dW{layer_n}\"] = partial_derivatire\n",
        "\n",
        "    return grads\n",
        "\n",
        "  def update(self, grads):\n",
        "    \"\"\"\n",
        "      grads: np.dictionary\n",
        "        Gradients obtained from the backward pass.\n",
        "\n",
        "      Results: Updates the network's weights and biases.\n",
        "    \"\"\"\n",
        "    if self.debug:print(f\"\\nUpdate ... ~~~~~~~~~~~~~~~~\")\n",
        "    for layer in range(1, self.n_hidden + 2):\n",
        "        # WRITE CODE HERE\n",
        "        if self.debug:print(f\"layer_{layer}\")\n",
        "\n",
        "        grad = grads[f\"dW{layer}\"]\n",
        "        weights = self.weights[f\"W{layer}\"]\n",
        "        if self.debug:print(f\"   weights_{weights.shape} - lr * grad_{grad.shape}\")\n",
        "        weights = weights - self.lr * grad\n",
        "        self.weights[f\"W{layer}\"] = weights\n",
        "\n",
        "        bias_up = grads[f\"db{layer}\"]\n",
        "        bias = self.weights[f\"b{layer}\"]\n",
        "        if self.debug:print(f\"   weights_{bias.shape} - lr * bias_up_{bias_up.shape}\")\n",
        "        bias = bias - self.lr * bias_up\n",
        "        self.weights[f\"b{layer}\"] = bias"
      ],
      "metadata": {
        "id": "wGcNlHs101Sb"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qyMqzTFyuAF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.random.rand(5, 784)\n",
        "labels = np.zeros((5, 10))\n",
        "labels[:,:1] = 1\n",
        "print(labels)\n",
        "print(f\"Input size: {x.shape}\")\n",
        "\n",
        "nn = NN()\n",
        "nn.debug = True\n",
        "nn.initialize_weights()\n",
        "res = nn.forward(x)\n",
        "output = res[\"Z3\"]\n",
        "print(output)\n",
        "training_loop = 1\n",
        "loss = nn.loss(output, labels)\n",
        "print(f\"Loss: {loss}\")\n",
        "\n",
        "for _ in range(training_loop):\n",
        "    res = nn.forward(x)\n",
        "    output = res[\"Z3\"]\n",
        "    loss = nn.loss(output, labels)\n",
        "    print(f\"Loss: {loss}\")\n",
        "\n",
        "    grads = nn.backward(res, labels)\n",
        "    nn.update(grads)\n",
        "\n",
        "res = nn.forward(x)\n",
        "output = res[\"Z3\"]\n",
        "print(output)\n",
        "\n",
        "np.argmax(output, axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQ9s1cGIoHAt",
        "outputId": "7864507b-c366-4575-d7b1-6ca10dc05c26"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "Input size: (5, 784)\n",
            "\n",
            "Foward ...\n",
            "layer_1\n",
            "    z_(5, 784) dot w_(784, 128) + b_(1, 128)\n",
            "    = z_(5, 128)\n",
            "layer_2\n",
            "    z_(5, 128) dot w_(128, 64) + b_(1, 64)\n",
            "    = z_(5, 64)\n",
            "layer_3\n",
            "    z_(5, 64) dot w_(64, 10) + b_(1, 10)\n",
            "    = z_(5, 10)\n",
            "[[0.09931704 0.09984055 0.09966514 0.10010934 0.10070109 0.09965174\n",
            "  0.10011623 0.09913298 0.10056654 0.10089934]\n",
            " [0.09861982 0.09952156 0.09981243 0.10005793 0.10038148 0.1002567\n",
            "  0.0999746  0.09990091 0.10073391 0.10074066]\n",
            " [0.0986791  0.09993813 0.09993374 0.10019995 0.10075545 0.09977775\n",
            "  0.0999143  0.09976137 0.10041154 0.10062866]\n",
            " [0.09824672 0.09960783 0.10002933 0.09993164 0.10039489 0.10027982\n",
            "  0.10029415 0.09969323 0.10058814 0.10093425]\n",
            " [0.09928766 0.09987312 0.09996769 0.09985034 0.10001318 0.10003813\n",
            "  0.09966909 0.09961099 0.10088057 0.10080925]]\n",
            "Loss: 2.3143621413221735\n",
            "\n",
            "Foward ...\n",
            "layer_1\n",
            "    z_(5, 784) dot w_(784, 128) + b_(1, 128)\n",
            "    = z_(5, 128)\n",
            "layer_2\n",
            "    z_(5, 128) dot w_(128, 64) + b_(1, 64)\n",
            "    = z_(5, 64)\n",
            "layer_3\n",
            "    z_(5, 64) dot w_(64, 10) + b_(1, 10)\n",
            "    = z_(5, 10)\n",
            "Loss: 2.3143621413221735\n",
            "\n",
            "Backward ...\n",
            "last layer ... layer_3 ----------\n",
            "   -> delta\n",
            "      delta = pred_(5, 10) - labels_(5, 10)\n",
            "      delta_(5, 10)\n",
            "   -> Update\n",
            "      partial_derivatire = z_in_(64,) * delta_(5, 10)\n",
            "      partial_derivatire_(64, 10)\n",
            "Looping backward ... layer_2 ----------\n",
            "   -> delta\n",
            "      delta_pre_act = weight_(10, 64) * delta_(10,)\n",
            "      delta_ = delta_pre_act_(64,) * activation_back_(64,)\n",
            "      delta_(64,)\n",
            "   -> Update\n",
            "      partial_derivatire = z_in_(128,) * delta_(64,)\n",
            "      partial_derivatire_(128, 64)\n",
            "Looping backward ... layer_1 ----------\n",
            "   -> delta\n",
            "      delta_pre_act = weight_(64, 128) * delta_(64,)\n",
            "      delta_ = delta_pre_act_(128,) * activation_back_(128,)\n",
            "      delta_(128,)\n",
            "   -> Update\n",
            "      partial_derivatire = z_in_(784,) * delta_(128,)\n",
            "      partial_derivatire_(784, 128)\n",
            "\n",
            "Update ... ~~~~~~~~~~~~~~~~\n",
            "layer_1\n",
            "   weights_(784, 128) - lr * grad_(784, 128)\n",
            "   weights_(1, 128) - lr * bias_up_(128,)\n",
            "layer_2\n",
            "   weights_(128, 64) - lr * grad_(128, 64)\n",
            "   weights_(1, 64) - lr * bias_up_(64,)\n",
            "layer_3\n",
            "   weights_(64, 10) - lr * grad_(64, 10)\n",
            "   weights_(1, 10) - lr * bias_up_(10,)\n",
            "\n",
            "Foward ...\n",
            "layer_1\n",
            "    z_(5, 784) dot w_(784, 128) + b_(1, 128)\n",
            "    = z_(5, 128)\n",
            "layer_2\n",
            "    z_(5, 128) dot w_(128, 64) + b_(1, 64)\n",
            "    = z_(5, 64)\n",
            "layer_3\n",
            "    z_(5, 64) dot w_(64, 10) + b_(1, 10)\n",
            "    = z_(5, 10)\n",
            "[[0.10042762 0.09971439 0.09950442 0.099974   0.10058121 0.09952017\n",
            "  0.10001942 0.09901666 0.10045212 0.10078997]\n",
            " [0.09972574 0.09940763 0.09966025 0.09991152 0.10027024 0.10010972\n",
            "  0.09988051 0.09979219 0.10060221 0.10063999]\n",
            " [0.09979551 0.0998173  0.09977467 0.10005982 0.10063441 0.09964126\n",
            "  0.09981035 0.09966038 0.10030297 0.10050333]\n",
            " [0.09936284 0.09949928 0.09986579 0.09979864 0.10027001 0.10014512\n",
            "  0.10018304 0.09958516 0.10047751 0.10081261]\n",
            " [0.10039223 0.09977281 0.09980906 0.09971093 0.09988721 0.09989523\n",
            "  0.09956461 0.09950315 0.10075891 0.10070588]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9, 9, 4, 9, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2: Implementing CNN layers with NumPy (20 points)\n",
        "Note: You may assume that there are no biases, no input padding (valid convolution) and also that convolution here refers to cross-correlation, i.e., no kernel flipping when convolving the inputs."
      ],
      "metadata": {
        "id": "bfDi9jm2nRpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Convolution2dLayer(object):\n",
        "  \"\"\"\n",
        "    Implements a 2D convolution layer.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, filter_size=3, stride=1, n_units=64, seed=0):\n",
        "    \"\"\"\n",
        "      Constructor of the Convolution2dLayer class.\n",
        "\n",
        "      Note: We assume that the input images have only a single channel.\n",
        "\n",
        "      filter_size: int, default 3\n",
        "        Filter size to use for convolution. We assume equal height and width.\n",
        "      stride: int, default 1\n",
        "        Stride for convolution.\n",
        "      n_units: int, default 64\n",
        "        Number of output channels, i.e., number of filters in the layer.\n",
        "      seed: int, default 0\n",
        "        Random seed.\n",
        "    \"\"\"\n",
        "    super(Convolution2dLayer, self).__init__()\n",
        "\n",
        "    self.filter_size = filter_size\n",
        "    self.stride = stride\n",
        "    self.n_units = n_units\n",
        "    self.seed = seed\n",
        "\n",
        "  def initialize_weights(self):     \n",
        "    \"\"\"\n",
        "      Results: Initializes the weights of the CNN from uniform(0, 1).\n",
        "    \"\"\"   \n",
        "    if self.seed is not None:\n",
        "      np.random.seed(self.seed)\n",
        "\n",
        "    # self.weights is an np.array of shape (n_units, filter_size, filter_size)\n",
        "    # We do not consider biases in this convolution layer implementation\n",
        "    # WRITE CODE HERE\n",
        "    self.weights = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "      x: np.array\n",
        "        Inputs to convolve. This may contain multiple input examples, not just one.\n",
        "        Note: We assume that the input images have only a single channel, e.g. (5, 1, 32, 32) where 5 is the number of\n",
        "        images, 1 channel, 32x32 image size.\n",
        "\n",
        "      Outputs: Inputs and the result of the convolution operation on the inputs stored in cache.\n",
        "\n",
        "      Note: You need not flip the kernel! You may just implement cross-correlation.\n",
        "    \"\"\"\n",
        "    cache = {}\n",
        "\n",
        "    # cache is a dictionary where cache[\"x\"] stores the inputs and cache[\"out\"] stores the outputs of the layer\n",
        "    # WRITE CODE HERE\n",
        "    cache[\"x\"] = x\n",
        "    cache[\"out\"] = None\n",
        "    pass\n",
        "    return cache\n",
        "\n",
        "  def backward(self, cache, grad_output):\n",
        "    \"\"\"\n",
        "      cache: dictionary\n",
        "        Contains the inputs and the result of the convolution operation applied on them.\n",
        "      grad_output: np.array\n",
        "        Gradient of the loss with respect to the outputs of the convolution layer.\n",
        "\n",
        "      Outputs: Gradient of the loss with respect to the parameters of the convolution layer.\n",
        "    \"\"\"\n",
        "    # grads is an np.array containing the gradient of the loss with respect to the parameters in the convolution layer\n",
        "    # Remember to account for the number of input examples!\n",
        "    # WRITE CODE HERE\n",
        "    grads = None\n",
        "    pass\n",
        "    return grads\n",
        "\n",
        "\n",
        "class MaxPooling2dLayer(object):\n",
        "  \"\"\"\n",
        "    Implements a 2D max-pooling layer.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, filter_size=2):\n",
        "    \"\"\"\n",
        "      Constructor of the MaxPooling2dLayer class.\n",
        "\n",
        "      filter_size: int, default 2\n",
        "        Filter size to use for max-pooling. We assume equal height and width, and stride = height = width.\n",
        "    \"\"\"\n",
        "    super(MaxPooling2dLayer, self).__init__()\n",
        "  \n",
        "    self.filter_size = filter_size\n",
        "    self.stride = filter_size\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "      x: np.array\n",
        "        Inputs to compute max-pooling for. This may contain multiple input examples, not just one.\n",
        "        Note: The input dimensions to max-pooling are the output dimensions of the convolution!\n",
        "\n",
        "      Outputs: Inputs and the result of the max-pooling operation on the inputs stored in cache.\n",
        "    \"\"\"\n",
        "    cache = {}\n",
        "\n",
        "    # cache is a dictionary where cache[\"x\"] stores the inputs and cache[\"out\"] stores the outputs of the layer\n",
        "    # WRITE CODE HERE\n",
        "    cache[\"x\"] = x\n",
        "    cache[\"out\"] = None\n",
        "    return cache\n",
        "\n",
        "  def backward(self, cache, grad_output):\n",
        "    \"\"\"\n",
        "      cache: dictionary\n",
        "        Contains the inputs and the result of the max-pooling operation applied on them.\n",
        "      grad_output: np.array\n",
        "        Gradient of the loss with respect to the outputs of the max-pooling layer.\n",
        "\n",
        "      Outputs: Gradient of the loss with respect to the inputs to the max-pooling layer.\n",
        "    \"\"\"\n",
        "    grads = None # WRITE CODE HERE (initialize grads correctly)\n",
        "\n",
        "    # grads is an np.array containing the gradient of the loss with respect to the inputs to the max-pooling layer\n",
        "    # Remember to account for the number of input examples!\n",
        "    # WRITE CODE HERE\n",
        "    pass\n",
        "    return grads"
      ],
      "metadata": {
        "id": "nE4N79Gondyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3: Implementing a CNN and comparison with MLPs using PyTorch (50 points)"
      ],
      "metadata": {
        "id": "AUMIWPZVYXp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY!\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Fix random seed\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "zWKKIED3CPsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "  \"\"\"This class implements the Residual Block used in ResNet-18.\"\"\"\n",
        "\n",
        "  def __init__(self, in_channels, channels, conv_stride=1, activation_str=\"relu\", initialization=\"xavier_normal\"):\n",
        "    \"\"\"\n",
        "      Constructor for the ResidualBlock class.\n",
        "\n",
        "      in_channels: int\n",
        "        Number of channels in the input to the block.\n",
        "      channels: int\n",
        "        Number of output channels for the block, i.e., number of filters.\n",
        "      conv_stride: int, default 1\n",
        "        Stride of the first convolution layer and downsampling convolution (if required).\n",
        "      activation_str: string, default \"relu\"\n",
        "        Activation function to use.\n",
        "      initialization: string, default \"xavier_normal\"\n",
        "        Initialization for convolution layer weights.\n",
        "    \"\"\"\n",
        "    super(ResidualBlock, self).__init__()\n",
        "\n",
        "    self.in_channels = in_channels\n",
        "    self.channels = channels\n",
        "    self.conv_stride = conv_stride\n",
        "    self.activation_str = activation_str\n",
        "    self.initialization = initialization\n",
        "\n",
        "    # Define these members by replacing `None` with the correct definitions\n",
        "    self.conv1 = None # WRITE CODE HERE\n",
        "    self.bn1 = None   # WRITE CODE HERE\n",
        "    self.conv2 = None # WRITE CODE HERE\n",
        "    self.bn2 = None   # WRITE CODE HERE\n",
        "\n",
        "    self.residual_connection = self.residual(in_channels, channels, conv_stride)\n",
        "\n",
        "    # Initialize weights for conv1 and conv2\n",
        "    if initialization == \"xavier_normal\":\n",
        "      # WRITE CODE HERE\n",
        "      pass\n",
        "    elif initialization == \"xavier_uniform\":\n",
        "      # WRITE CODE HERE\n",
        "      pass\n",
        "    elif initialization == \"kaiming_normal\":\n",
        "      # WRITE CODE HERE\n",
        "      pass\n",
        "    else:\n",
        "      raise Exception(\"Invalid initialization\")\n",
        "\n",
        "  def activation(self, input):\n",
        "    \"\"\"\n",
        "      input: Tensor\n",
        "        Input on which the activation is applied.\n",
        "\n",
        "      Output: Result of activation function applied on input.\n",
        "        E.g. if self.activation_str is \"relu\", return relu(input).\n",
        "    \"\"\"\n",
        "    if self.activation_str == \"relu\":\n",
        "      # WRITE CODE HERE\n",
        "      pass\n",
        "    elif self.activation_str == \"tanh\":\n",
        "      # WRITE CODE HERE\n",
        "      pass\n",
        "    else:\n",
        "      raise Exception(\"Invalid activation\")\n",
        "    return 0\n",
        "\n",
        "  def residual(self, in_channels, channels, conv_stride=1):\n",
        "    \"\"\"\n",
        "      in_channels: int\n",
        "        Number of input channels in the input to the block.\n",
        "      channels: int\n",
        "        Number of output channels for the block, i.e., number of filters.\n",
        "      conv_stride: int, default 1\n",
        "        Stride to use for downsampling 1x1 convolution.\n",
        "\n",
        "      Output: Returns an nn.Sequential object which computes the identity function of the input if stride is 1\n",
        "              and the number of input channels equals the number of output channels. Otherwise, it returns an\n",
        "              nn.Sequential object that downsamples its input using a 1x1-conv of the stride specified and\n",
        "              followed by a BatchNorm2d.\n",
        "    \"\"\"\n",
        "    layers = []\n",
        "    if conv_stride != 1 or in_channels != channels:\n",
        "      # WRITE CODE HERE\n",
        "      pass\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "      x: Tensor\n",
        "        Input to the block.\n",
        "\n",
        "      Outputs: Returns the output of the forward pass of the block.\n",
        "    \"\"\"\n",
        "    # WRITE CODE HERE\n",
        "    pass\n",
        "    return 0"
      ],
      "metadata": {
        "id": "ntEJKZ2eYgqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet18(nn.Module):\n",
        "  \"\"\"This class implements the ResNet-18 architecture from its components.\"\"\"\n",
        "\n",
        "  def __init__(self, activation_str=\"relu\", initialization=\"xavier_normal\"):\n",
        "    \"\"\"\n",
        "      Constructor for the ResNet18 class.\n",
        "\n",
        "      activation_str: string, default \"relu\"\n",
        "        Activation function to use.\n",
        "      initialization: string, default \"xavier_normal\"\n",
        "        Weight initialization to use.\n",
        "    \"\"\"\n",
        "    super(ResNet18, self).__init__()\n",
        "\n",
        "    self.n_classes = 10\n",
        "    self.activation_str = activation_str\n",
        "    self.initialization = initialization\n",
        "\n",
        "    # Define these members by replacing `None` with the correct definitions\n",
        "    self.conv1 = None   # WRITE CODE HERE\n",
        "    self.bn1 = None     # WRITE CODE HERE\n",
        "    self.layer1 = None  # WRITE CODE HERE (use _create_layer)\n",
        "    self.layer2 = None  # WRITE CODE HERE (use _create_layer)\n",
        "    self.layer3 = None  # WRITE CODE HERE (use _create_layer)\n",
        "    self.layer4 = None  # WRITE CODE HERE (use _create_layer)\n",
        "    self.avgpool = None # WRITE CODE HERE\n",
        "    self.linear = None  # WRITE CODE HERE\n",
        "  \n",
        "  def activation(self, input):\n",
        "    \"\"\"\n",
        "      input: Tensor\n",
        "        Input on which the activation is applied.\n",
        "\n",
        "      Output: Result of activation function applied on input.\n",
        "        E.g. if self.activation_str is \"relu\", return relu(input).\n",
        "    \"\"\"\n",
        "    if self.activation_str == \"relu\":\n",
        "      # WRITE CODE HERE\n",
        "      pass\n",
        "    elif self.activation_str == \"tanh\":\n",
        "      # WRITE CODE HERE\n",
        "      pass\n",
        "    else:\n",
        "      raise Exception(\"Invalid activation\")\n",
        "    return 0\n",
        "\n",
        "  def _create_layer(self, in_channels, channels, conv_stride=1):\n",
        "    \"\"\"\n",
        "      in_channels: int\n",
        "        Number of input channels present in the input to the layer.\n",
        "      out_channels: int\n",
        "        Number of output channels for the layer, i.e., the number of filters.\n",
        "      conv_stride: int, default 1\n",
        "        Stride of the first convolution layer in the block and the downsampling convolution (if required).\n",
        "\n",
        "      Outputs: Returns an nn.Sequential object giving a \"layer\" of the ResNet, consisting of 2 blocks each.\n",
        "    \"\"\"\n",
        "    # Modify the following statement to return an nn.Sequential object containing 2 ResidualBlocks.\n",
        "    # You must make sure that the appropriate channels and conv_stride are provided.\n",
        "    return nn.Sequential() # WRITE CODE HERE\n",
        "\n",
        "  def get_first_conv_layer_filters(self):\n",
        "    \"\"\"\n",
        "      Outputs: Returns the filters in the first convolution layer.\n",
        "    \"\"\"\n",
        "    return self.conv1.weight.clone().cpu().detach().numpy()\n",
        "\n",
        "  def get_last_conv_layer_filters(self):\n",
        "    \"\"\"\n",
        "      Outputs: Returns the filters in the last convolution layer.\n",
        "    \"\"\"\n",
        "    return list(self.layer4.modules())[1].conv2.weight.clone().cpu().detach().numpy()\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "      x: Tensor\n",
        "        Input to the network.\n",
        "\n",
        "      Outputs: Returns the output of the forward pass of the network.\n",
        "    \"\"\"\n",
        "    # WRITE CODE HERE\n",
        "    pass\n",
        "    return 0"
      ],
      "metadata": {
        "id": "o0XI3Qo1nMEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cifar10():  \n",
        "  transform = transforms.Compose([\n",
        "      transforms.ToTensor()\n",
        "  ])\n",
        "\n",
        "  train_dataset = torchvision.datasets.CIFAR10(\n",
        "      root='./data', train=True, download=True, transform=transform)\n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "      train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "  val_dataset = torchvision.datasets.CIFAR10(\n",
        "      root='./data', train=False, download=True, transform=transform)\n",
        "  val_loader = torch.utils.data.DataLoader(\n",
        "      val_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
        "  \n",
        "  return train_loader, val_loader\n",
        "\n",
        "def train_loop(epoch, model, train_loader, criterion, optimizer):\n",
        "  \"\"\"\n",
        "    epoch: int\n",
        "      Number of the current training epoch (starting from 0).\n",
        "    model: ResNet18\n",
        "      The model to train, which is an instance of the ResNet18 class.\n",
        "    train_loader: DataLoader\n",
        "      The training dataloader.\n",
        "    criterion: Module\n",
        "      A Module object that evaluates the crossentropy loss.\n",
        "    optimizer: Optimizer\n",
        "      An Optimizer object for the Adam optimizer.\n",
        "\n",
        "    Outputs: Returns average train_acc and train_loss for the current epoch.\n",
        "  \"\"\"\n",
        "  train_acc = 0.\n",
        "  train_loss = 0.\n",
        "\n",
        "  # WRITE CODE HERE\n",
        "\n",
        "  print(f\"Epoch: {epoch} | Train Acc: {train_acc:.6f} | Train Loss: {train_loss:.6f}\")\n",
        "  return train_acc, train_loss\n",
        "\n",
        "def valid_loop(epoch, model, val_loader, criterion):\n",
        "  \"\"\"\n",
        "    epoch: int\n",
        "      Number of the current epoch (starting from 0).\n",
        "    model: ResNet18\n",
        "      The model to train, which is an instance of the ResNet18 class.\n",
        "    val_loader: DataLoader\n",
        "      The validation dataloader.\n",
        "    criterion: Module\n",
        "      A Module object that evaluates the crossentropy loss.\n",
        "\n",
        "    Outputs: Returns average val_acc and val_loss for the current epoch.\n",
        "  \"\"\"\n",
        "  val_acc = 0.\n",
        "  val_loss = 0.\n",
        "\n",
        "  # WRITE CODE HERE\n",
        "\n",
        "  print(f\"Epoch: {epoch} | Val Acc: {val_acc:.6f}   | Val Loss: {val_loss:.6f}\")\n",
        "  return val_acc, val_loss"
      ],
      "metadata": {
        "id": "VTImAF6S7xIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "activation_str = \"relu\"\n",
        "initialization = \"xavier_normal\""
      ],
      "metadata": {
        "id": "8fsB_xpIC1oG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  train_accs, train_losses, val_accs, val_losses = [], [], [], []\n",
        "  n_epochs = 25\n",
        "\n",
        "  model = ResNet18(\n",
        "    activation_str=activation_str,\n",
        "    initialization=initialization\n",
        "  ).to(device)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "  train_loader, val_loader = get_cifar10()\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "    # Training\n",
        "    train_acc, train_loss = train_loop(epoch, model, train_loader, criterion, optimizer)\n",
        "    train_accs.append(train_acc)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Validation\n",
        "    val_acc, val_loss = valid_loop(epoch, model, val_loader, criterion)\n",
        "    val_accs.append(val_acc)\n",
        "    val_losses.append(val_loss)"
      ],
      "metadata": {
        "id": "Fr4bZ8W1B6Kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Questions 3.4, 3.5, 3.6, 3.7, 3.8\n",
        "You may write your own code for these questions below. These will not be autograded and you need not submit code for these, only the report."
      ],
      "metadata": {
        "id": "AAtU_NnOmzl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For Q 3.6\n",
        "if __name__ == \"__main__\":\n",
        "  vis_image = None\n",
        "  for data, labels in val_loader:\n",
        "    vis_image = data[12].unsqueeze(0)\n",
        "    break"
      ],
      "metadata": {
        "id": "9UcxsPr-hyNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can also upload `vis_image.pkl` from Piazza and use it:\n",
        "# import pickle\n",
        "# vis_image = pickle.load(open(\"vis_image.pkl\", \"rb\")).to(device)\n",
        "# plt.imshow(vis_image.squeeze().permute(1, 2, 0).cpu().detach().numpy())"
      ],
      "metadata": {
        "id": "lrGp-g97loFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# plt.imshow(vis_image.squeeze().permute(1, 2, 0).cpu().detach().numpy())"
      ],
      "metadata": {
        "id": "eJpCIBVolmZk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}